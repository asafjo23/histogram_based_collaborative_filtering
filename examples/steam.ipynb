{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from pandas import read_csv\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from config import DATA_DIR, MODELS_DIR\n",
    "from src.loss import Loss\n",
    "from src.model import HistogramMF\n",
    "from src.runner import Runner\n",
    "from src.create_dataset import create_dataset\n",
    "from src.data_processor import DataProcessor\n",
    "from src.data_encoder import DataEncoder\n",
    "\n",
    "DF_PATH = f\"{DATA_DIR}/Steam/steam-200k_with_histogram_features.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "       user_id            item_id  rating  original_mass  total_mass\n1.0  151603712          Fallout 4     1.0           32.0        65.0\n2.0  151603712          Fallout 4    87.0           63.5        65.0\n3.0  151603712              Spore     1.0           32.0        65.0\n4.0  151603712              Spore    14.9           63.0        65.0\n5.0  151603712  Fallout New Vegas     1.0           32.0        65.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n      <th>original_mass</th>\n      <th>total_mass</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1.0</th>\n      <td>151603712</td>\n      <td>Fallout 4</td>\n      <td>1.0</td>\n      <td>32.0</td>\n      <td>65.0</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>151603712</td>\n      <td>Fallout 4</td>\n      <td>87.0</td>\n      <td>63.5</td>\n      <td>65.0</td>\n    </tr>\n    <tr>\n      <th>3.0</th>\n      <td>151603712</td>\n      <td>Spore</td>\n      <td>1.0</td>\n      <td>32.0</td>\n      <td>65.0</td>\n    </tr>\n    <tr>\n      <th>4.0</th>\n      <td>151603712</td>\n      <td>Spore</td>\n      <td>14.9</td>\n      <td>63.0</td>\n      <td>65.0</td>\n    </tr>\n    <tr>\n      <th>5.0</th>\n      <td>151603712</td>\n      <td>Fallout New Vegas</td>\n      <td>1.0</td>\n      <td>32.0</td>\n      <td>65.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"user_id\", \"item_id\", \"action\", \"rating\", \"null\", \"original_mass\", \"total_mass\"]\n",
    "original_df = read_csv(DF_PATH, skipinitialspace=True, names=columns, low_memory=False)\n",
    "\n",
    "original_df = original_df[[\"user_id\", \"item_id\", \"rating\", \"original_mass\", \"total_mass\"]]\n",
    "original_df = original_df[1:][:]\n",
    "\n",
    "original_df = original_df.astype({\"user_id\": \"int32\"})\n",
    "original_df = original_df.astype({\"item_id\": str})\n",
    "original_df = original_df.astype({\"rating\": \"float64\"})\n",
    "original_df = original_df.astype({\"original_mass\": \"float64\"})\n",
    "original_df = original_df.astype({\"total_mass\": \"float64\"})\n",
    "\n",
    "original_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 14/200 [00:07<01:44,  1.77batch/s, train_loss=inf]     \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 31>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SummaryWriter(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mruns/steam/dev\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m writer:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m---> 33\u001B[0m         epoch_loss \u001B[38;5;241m=\u001B[39m \u001B[43mrunner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_load\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwriter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     35\u001B[0m         torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mMODELS_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/steam/model.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/Academy/Vision/Project/histogram_based_collaborative_filtering/src/runner.py:28\u001B[0m, in \u001B[0;36mRunner.train\u001B[0;34m(self, train_loader, epoch, writer)\u001B[0m\n\u001B[1;32m     25\u001B[0m original_mass \u001B[38;5;241m=\u001B[39m original_mass\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m     26\u001B[0m total_mass \u001B[38;5;241m=\u001B[39m total_mass\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m---> 28\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43musers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43musers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitems\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m predicted_rating \u001B[38;5;241m=\u001B[39m p[:, :\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m     31\u001B[0m predicted_mass \u001B[38;5;241m=\u001B[39m p[:, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39msqueeze()\n",
      "File \u001B[0;32m~/Desktop/Academy/Vision/Project/histogram_based_collaborative_filtering/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Academy/Vision/Project/histogram_based_collaborative_filtering/src/model.py:52\u001B[0m, in \u001B[0;36mHistogramMF.forward\u001B[0;34m(self, users, items)\u001B[0m\n\u001B[1;32m     50\u001B[0m original_ratings_by_user[original_song_index] \u001B[38;5;241m=\u001B[39m prediction\n\u001B[1;32m     51\u001B[0m predicted_round_rating \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mround(prediction)\n\u001B[0;32m---> 52\u001B[0m predicted_rating_index \u001B[38;5;241m=\u001B[39m \u001B[43m_to_index\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmin_rating\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_min_rating\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_rating\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_max_rating\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrating\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpredicted_round_rating\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m predicted_histogram \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mhistc(\n\u001B[1;32m     59\u001B[0m     original_ratings_by_user,\n\u001B[1;32m     60\u001B[0m     bins\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mround\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_rating),\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mround\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_min_rating),\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28mmax\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mround\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_rating),\n\u001B[1;32m     63\u001B[0m )\n\u001B[1;32m     64\u001B[0m predicted_mass \u001B[38;5;241m=\u001B[39m _calc_histogram_mass(predicted_histogram, predicted_rating_index)\n",
      "File \u001B[0;32m~/Desktop/Academy/Vision/Project/histogram_based_collaborative_filtering/src/model.py:74\u001B[0m, in \u001B[0;36m_to_index\u001B[0;34m(min_rating, max_rating, rating)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_index\u001B[39m(min_rating: \u001B[38;5;28mint\u001B[39m, max_rating: \u001B[38;5;28mint\u001B[39m, rating: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m     73\u001B[0m     min_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(min_rating \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 74\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclip\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrating\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mmin\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmin_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mmax\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_rating\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "data_encoder = DataEncoder(original_df=original_df)\n",
    "data_processor = DataProcessor(original_df=original_df)\n",
    "\n",
    "n_users = original_df.user_id.nunique()\n",
    "n_items = original_df.item_id.nunique()\n",
    "\n",
    "min_rating = min(original_df.rating.values)\n",
    "max_rating = max(original_df.rating.values)\n",
    "\n",
    "model = HistogramMF(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    data_encoder=data_encoder,\n",
    "    data_processor=data_processor,\n",
    "    min_rating=min_rating,\n",
    "    max_rating=max_rating,\n",
    ")\n",
    "\n",
    "if os.path.exists(f\"{MODELS_DIR}/steam/model.pt\"):\n",
    "    model.load_state_dict(torch.load(f\"{MODELS_DIR}/steam/model.pt\"))\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "criterion = Loss()\n",
    "optimizer = SGD(model.parameters(), lr=5, weight_decay=1e-7)\n",
    "runner = Runner(model=model, criterion=criterion, optimizer=optimizer)\n",
    "\n",
    "train_set = create_dataset(data_encoder=data_encoder)\n",
    "train_load = DataLoader(train_set, batch_size=1000, shuffle=True)\n",
    "\n",
    "with SummaryWriter(f\"runs/steam/dev\") as writer:\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = runner.train(train_loader=train_load, epoch=epoch, writer=writer)\n",
    "        print(f\"epoch={epoch + 1}, loss={epoch_loss}\")\n",
    "        torch.save(model.state_dict(), f\"{MODELS_DIR}/steam/model.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from collections import namedtuple\n",
    "\n",
    "Row = namedtuple(\"Row\", \"user_id item_id rating\")\n",
    "\n",
    "data_encoder = DataEncoder(original_df=original_df)\n",
    "dataframe_after_mf = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (index, user_id, item_id, rating, _, _) in original_df.itertuples():\n",
    "        encoded_user_id = data_encoder.get_encoded_user_id(original_id=user_id)\n",
    "        encoded_item_id = data_encoder.get_encoded_item_id(original_id=item_id)\n",
    "\n",
    "        user_id_as_tensor = torch.LongTensor([encoded_user_id])\n",
    "        item_id_as_tensor = torch.LongTensor([encoded_item_id])\n",
    "        output = model(users=user_id_as_tensor, items=item_id_as_tensor,).squeeze()[0]\n",
    "        predicted_rating = torch.round(output).item()\n",
    "\n",
    "        dataframe_after_mf.append(Row(user_id=user_id, item_id=item_id, rating=predicted_rating))\n",
    "\n",
    "df_after_mf = DataFrame(dataframe_after_mf, columns=[\"user_id\", \"item_id\", \"rating\"])\n",
    "df_after_mf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compare = original_df[[\"user_id\", \"item_id\", \"rating\"]].reset_index(drop=True)\n",
    "df_after_mf = df_after_mf.reset_index(drop=True)\n",
    "# df_after_mf.head()\n",
    "mask = (compare[\"rating\"] == df_after_mf[\"rating\"])\n",
    "changes = compare[mask].copy()\n",
    "changes[\"New rating\"] = df_after_mf.rating\n",
    "print(f\"Number of hits: {len(changes)} / {len(original_df)}\")\n",
    "changes.head(len(changes))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}